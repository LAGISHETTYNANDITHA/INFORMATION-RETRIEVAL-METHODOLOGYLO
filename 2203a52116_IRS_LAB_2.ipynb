{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "# Install NLTK if not already installed\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import os\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Cats running faster than mice have often puzzled scientists.\"\n",
        "\n",
        "# Task 1: Tokenization\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(\"Tokenized words:\", tokens)\n",
        "\n",
        "# Task 2: Stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "porter_stemmed = [porter_stemmer.stem(token) for token in tokens]\n",
        "snowball_stemmed = [snowball_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"\\nPorter Stemmer Results:\", porter_stemmed)\n",
        "print(\"Snowball Stemmer Results:\", snowball_stemmed)\n",
        "\n",
        "# Task 3: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(token, wordnet.VERB) for token in tokens]\n",
        "print(\"\\nLemmatized words:\", lemmatized_words)\n",
        "\n",
        "# Save the results to a file\n",
        "filename = \"<HallticketNo>-<BatchNo>-Lab-2.ipynb\"\n",
        "with open(filename, \"w\") as file:\n",
        "    file.write(\"# Tokenization\\n\")\n",
        "    file.write(f\"Tokenized words: {tokens}\\n\\n\") # Corrected indentation\n",
        "\n",
        "    file.write(\"# Stemming\\n\")\n",
        "    file.write(f\"Porter Stemmer Results: {porter_stemmed}\\n\") # Corrected indentation\n",
        "    file.write(f\"Snowball Stemmer Results: {snowball_stemmed}\\n\\n\") # Corrected indentation\n",
        "\n",
        "    file.write(\"# Lemmatization\\n\")\n",
        "    file.write(f\"Lemmatized words: {lemmatized_words}\\n\") # Corrected indentation\n",
        "\n",
        "    print(f\"\\nResults saved in {filename}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx6JFqwknR1q",
        "outputId": "185d912e-7e4e-48a5-a302-8291b7a4383c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['Cats', 'running', 'faster', 'than', 'mice', 'have', 'often', 'puzzled', 'scientists', '.']\n",
            "\n",
            "Porter Stemmer Results: ['cat', 'run', 'faster', 'than', 'mice', 'have', 'often', 'puzzl', 'scientist', '.']\n",
            "Snowball Stemmer Results: ['cat', 'run', 'faster', 'than', 'mice', 'have', 'often', 'puzzl', 'scientist', '.']\n",
            "\n",
            "Lemmatized words: ['Cats', 'run', 'faster', 'than', 'mice', 'have', 'often', 'puzzle', 'scientists', '.']\n",
            "\n",
            "Results saved in <HallticketNo>-<BatchNo>-Lab-2.ipynb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    }
  ]
}